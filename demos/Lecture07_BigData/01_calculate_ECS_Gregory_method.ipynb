{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Equilibrium Climate Sensitivity (ECS) in CMIP6 models\n",
    "\n",
    "*Definition:* Equilibrium Climate Sensitivity is defined as change in global-mean near-surface air temperature (GMST) change due to an instantaneous doubling of CO$_{2}$ concentrations and once the coupled ocean-atmosphere-sea ice system has acheived a statistical equilibrium (i.e. at the top-of-atmosphere, incoming solar shortwave radiation is balanced by reflected solar shortwave and outgoing thermal longwave radiation).\n",
    "\n",
    "This notebook uses the [\"Gregory method\"](https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2003GL018747) to approximate the ECS of CMIP6 models based on the first 150 years after an abrupt quadrupling of CO$_{2}$ concentrations. The \"Gregory Method\" extrapolates the quasi-linear relationship between GMST and radiative imbalance at the top-of-atmosphere to estimate how much warming would occur if the system were in radiative balance at the top-of-atmosphere, which is by definition the equilibrium response. In particular, we extrapolate the linear relationship that occurs between 100 and 150 years after the abrupt quadrupling. Since the radiative forcing due to CO$_{2}$ is a logarithmic function of the CO$_{2}$ concentration, the GMST change from a first doubling is roughly the same as for a second doubling (to first order, we can assume feedbacks as constant), which means that the GMST change due to a quadrupling of CO$_{2}$ is roughly $\\Delta T_{4 \\times \\text{CO}_{2}} = 2 \\times \\text{ECS}$. See also [Mauritsen et al. 2019](https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2018MS001400) for a detailed application of the Gregory Method (with modifications) for the case of one specific CMIP6 model, the MPI-M Earth System Model.\n",
    "\n",
    "For another take on applying the Gregory method to estimate ECS, see [Angeline Pendergrass' code](https://github.com/apendergrass/cmip6-ecs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolate linear relationship between GMST and radiative imbalance\n",
    "# between gregory_limits[0] and gregory_limits[1]\n",
    "gregory_limits = [0,150] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import cartopy\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "import intake\n",
    "# util.py is in the local directory\n",
    "# it contains code that is common across project notebooks\n",
    "# or routines that are too extensive and might otherwise clutter\n",
    "# the notebook design\n",
    "import util\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data catalogs\n",
    "\n",
    "This notebook uses [`intake-esm`](https://intake-esm.readthedocs.io/en/latest/) to ingest and organize climate model output from the fresh-off-the-supercomputers Phase 6 of the Coupled Model Intercomparison Project (CMIP6). \n",
    "\n",
    "The file `https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv` in google cloud storage contains thousands of lines of metadata, each describing an individual climate model experiment's simulated data.\n",
    "\n",
    "For example, the first line in the csv file contains the precipitation rate (`variable_id = 'pr'`), as a function of latitude, longitude, and time, in an individual climate model experiment with the BCC-ESM1 model (`source_id = 'BCC-ESM1'`) developed by the Beijing Climate Center (`institution_id = 'BCC'`). The model is *forced* by the forcing experiment SSP370 (`experiment_id = 'ssp370'`), which stands for the Shared Socio-Economic Pathway 3 that results in a change in radiative forcing of $\\Delta F = 7.0$ W/m$^{2}$ from pre-industrial to 2100. This simulation was run as part of the `AerChemMIP` activity, which is a spin-off of the CMIP activity that focuses specifically on how aerosol chemistry affects climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `pangeo-cmip6.json` describes the structure of the CMIP6 metadata and is formatted so as to be read in by the `intake.open_esm_datastore` method, which categorizes all of the data pointers into a tiered collection. For example, this collection contains the simulated data from 28691 individual experiments, representing 48 different models from 23 different scientific institutions. There are 190 different climate variables (e.g. sea surface temperature, sea ice concentration, atmospheric winds, dissolved organic carbon in the ocean, etc.) available for 29 different forcing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\"../catalogs/pangeo-cmip6.json\")\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we show the various forcing experiments that climate modellers ran in these simulations. A few examples are:\n",
    " - `piControl` which fixes CO2 levels at pre-industrial concentrations of 300 ppm\n",
    " - `historical` which includes the historical evolution of greenhouse concentrations as well as historical volcanic eruptions, changes in solar luminosity, and changes in atmospheric aerosol concentrations (and some other, less impactful forcings).\n",
    " - `abrupt-4xCO2` in which the CO2 concentrations in an pre-industrial control simulation are abrupted quadrupled from 300 ppm to 1200 ppm.\n",
    " - `ssp585`, a `worst-case scenario` in which fossil-fueled development leads to a disastrous increase of $\\Delta F = 8.5$ W/m$^{2}$ in radiative forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['experiment_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Climate Model Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "`intake-esm` enables loading data directly into an [xarray.DataArray](http://xarray.pydata.org/en/stable/api.html#dataset), a metadata-aware extension of numpy arrays. `xarray` objects leverage [dask](https://dask.org/) to only read data into memory as needed for any specific operation (i.e. lazy evaluation). Think of `xarray` Datasets as ways of conveniently organizing large arrays of floating point numbers (e.g. climate model data) on an n-dimensional discrete grid, with important metadata such as units, variable, names, etc.\n",
    "\n",
    "Note that data on the cloud are in [zarr](https://zarr.readthedocs.io/en/stable/) format, an extension of the metadata-aware format [netcdf](https://www.unidata.ucar.edu/software/netcdf/) commonly used in geosciences.\n",
    "\n",
    "`intake-esm` has rules for aggegating datasets; these rules are defined in the collection-specification file.\n",
    "\n",
    "#### Choice of simulated forcing experiments\n",
    "\n",
    "Here, we choose the `piControl` experiment (in which CO2 concentrations are held fixed at a pre-industrial level of ~300 ppm) and `abrupt-4xCO2` experiment (in which CO2 concentrations are instantaneously quadrupled - or doubled twice - from a pre-industrial controrl state). Since the radiative forcing of CO2 is roughly a logarithmic function of CO2 concentrations, the ECS is roughly independent of the initial CO2 concentration. Thus, if one doubling of CO2 results in $ECS$ of warming, then two doublings (or, a quadrupling) results in $2 \\times ECS$ of warming.\n",
    "\n",
    "Ideally, we would choose the `abrupt-2xCO2` forcing experiment, but this seems to be currently unavaiable in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tas = col.search(experiment_id=['abrupt-4xCO2','piControl'],  # pick the `abrupt-4xCO2` and `piControl` forcing experiments\n",
    "                 table_id='Amon',             # choose to look at atmospheric variables (A) saved at monthly resolution (mon)\n",
    "                 variable_id='tas',           # choose to look at near-surface air temperature (tas) as our variable\n",
    "                 member_id = 'r1i1p1f1')      # arbitrarily pick one realization for each model (i.e. just one set of initial conditions)\n",
    "\n",
    "cat_rad = col.search(experiment_id=['abrupt-4xCO2','piControl'],  # pick the `abrupt-4xCO2` and `piControl` forcing experiments\n",
    "                 table_id='Amon',             # choose to look at atmospheric variables (A) saved at monthly resolution (mon)\n",
    "                 variable_id=['rsut','rsdt','rlut'],           # choose to look at near-surface air temperature (tas) as our variable\n",
    "                 member_id = 'r1i1p1f1')      # arbitrarily pick one realization for each model (i.e. just one set of initial conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data catalog into a dictionary of xarray datasets\n",
    "dset_dict_tas = cat_tas.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': False})\n",
    "dset_dict_rad = cat_rad.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dset_dict = dict(dset_dict_tas, **dset_dict_rad)\n",
    "\n",
    "ds_dict = {}\n",
    "gmst_dict = {}\n",
    "imbalance_dict = {}\n",
    "for name, ds_rad in tqdm(dset_dict_rad.items()):\n",
    "    model_name = name.split(\".\")[2]\n",
    "    \n",
    "    if not (('rsdt' in dset_dict_rad[name].keys()) & ('rsut' in dset_dict_rad[name].keys()) & ('rlut' in dset_dict_rad[name].keys())):\n",
    "        continue\n",
    "        \n",
    "    ds_tas = dset_dict_tas[name]\n",
    "    \n",
    "    # rename spatial dimensions if necessary\n",
    "    if ('longitude' in ds_rad.dims) and ('latitude' in ds_rad.dims):\n",
    "        ds_rad = ds_rad.rename({'longitude':'lon', 'latitude': 'lat'}) # some models labelled dimensions differently...\n",
    "        ds_tas = ds_tas.rename({'longitude':'lon', 'latitude': 'lat'}) # some models labelled dimensions differently...\n",
    "        \n",
    "    ds_rad = xr.decode_cf(ds_rad) # temporary hack, not sure why I need this but has to do with calendar-aware metadata on the time variable\n",
    "    ds_tas = xr.decode_cf(ds_tas)\n",
    "    \n",
    "    # drop redundant variables (like \"height: 2m\")\n",
    "    for coord in ds_tas.coords:\n",
    "        if coord not in ['lat','lon','time']:\n",
    "            ds_tas = ds_tas.drop(coord)\n",
    "            \n",
    "    # drop redundant variables (like \"height: 2m\")\n",
    "    for coord in ds_rad.coords:\n",
    "        if coord not in ['lat','lon','time']:\n",
    "            ds_rad = ds_rad.drop(coord)\n",
    "\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(ds_tas['lat'])) * xr.ones_like(ds_tas['lon']) # effective area weights\n",
    "    gmst = (\n",
    "        (ds_tas['tas'] * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "        cos_lat_2d.sum(dim=['lat','lon'])\n",
    "    )\n",
    "    \n",
    "    ## Calculate global-mean top of atmosphere radiative imbalance (GMST)\n",
    "    net_toa = ds_rad['rsdt'] - ds_rad['rsut'] - ds_rad['rlut']\n",
    "    \n",
    "    cos_lat_2d = np.cos(np.deg2rad(ds_rad['lat'])) * xr.ones_like(ds_rad['lon']) # effective area weights\n",
    "    imbalance = (\n",
    "        (net_toa * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "        cos_lat_2d.sum(dim=['lat','lon'])\n",
    "    )\n",
    "    \n",
    "    imbalance_dict[name] = imbalance.squeeze()\n",
    "\n",
    "    # Add GMST to dictionary\n",
    "    gmst_dict[name] = gmst.squeeze()\n",
    "    \n",
    "    # Add near-surface air temperature to dictionary\n",
    "    ds_dict[name] = ds_tas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dictionary of all simulations into `piControl` and `abrupt-4xCO2` dictionaries\n",
    "\n",
    "And get rid of any models that don't have both experiments available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_dict = {} # dictionary that will hold spliced DataArrays\n",
    "for name, ds in ds_dict.items(): # Loop through dictionary\n",
    "    model_name = name.split(\".\")[2]\n",
    "    if 'piControl' not in name: continue\n",
    "    ctrl_dict[model_name] = ds\n",
    "    \n",
    "abrupt_dict = {} # dictionary that will hold spliced DataArrays\n",
    "for name, ds in ds_dict.items(): # Loop through dictionary\n",
    "    model_name = name.split(\".\")[2]\n",
    "    if ('abrupt-4xCO2' not in name) or (model_name not in ctrl_dict.keys()): continue\n",
    "    abrupt_dict[model_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_gmst_dict = {} # dictionary that will hold spliced DataArrays\n",
    "ctrl_imbalance_dict = {} # dictionary that will hold spliced DataArrays\n",
    "for name, gmst in gmst_dict.items(): # Loop through dictionary\n",
    "    model_name = name.split(\".\")[2]\n",
    "    if 'piControl' not in name: continue # keep only SSP simulations\n",
    "    ctrl_gmst_dict[model_name] = gmst\n",
    "    ctrl_imbalance_dict[model_name] = imbalance_dict[name]\n",
    "    \n",
    "abrupt_gmst_dict = {} # dictionary that will hold spliced DataArrays\n",
    "abrupt_imbalance_dict = {} # dictionary that will hold spliced DataArrays\n",
    "for name, gmst in gmst_dict.items(): # Loop through dictionary\n",
    "    model_name = name.split(\".\")[2]\n",
    "    if ('abrupt-4xCO2' not in name) or (model_name not in ctrl_gmst_dict.keys()): continue # keep only SSP simulations\n",
    "    abrupt_gmst_dict[model_name] = gmst\n",
    "    abrupt_imbalance_dict[model_name] = imbalance_dict[name]\n",
    "    \n",
    "ctrl_gmst_dict = {} # dictionary that will hold spliced DataArrays\n",
    "ctrl_imbalance_dict = {} # dictionary that will hold spliced DataArrays\n",
    "for name, gmst in gmst_dict.items(): # Loop through dictionary\n",
    "    model_name = name.split(\".\")[2]\n",
    "    if ('piControl' not in name) or (model_name not in abrupt_gmst_dict.keys()): continue # keep only SSP simulations\n",
    "    ctrl_gmst_dict[model_name] = gmst\n",
    "    ctrl_imbalance_dict[model_name] = imbalance_dict[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick first model arbitrarily for example application of Gregory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'CESM2'\n",
    "for name in ctrl_gmst_dict.keys():\n",
    "    if model == name:\n",
    "        ctrl_gmst = ctrl_gmst_dict[name].groupby('time.year').mean('time').compute()\n",
    "        ctrl_imbalance = ctrl_imbalance_dict[name].groupby('time.year').mean('time').compute()\n",
    "        abrupt_gmst = abrupt_gmst_dict[name].groupby('time.year').mean('time').compute() - ctrl_gmst.mean(dim='year')\n",
    "        abrupt_imbalance = abrupt_imbalance_dict[name].groupby('time.year').mean('time').compute() - ctrl_imbalance.mean(dim='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3.5))\n",
    "plt.subplot(1,2,1)\n",
    "abrupt_gmst.plot()\n",
    "plt.ylabel('GMST ($^{\\circ}C$)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "abrupt_imbalance.plot()\n",
    "plt.ylabel('radiative imbalance (W/m$^{2}$)');\n",
    "plt.savefig('../figures/4xCO2_temperature_evolution.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(12,3.5))\n",
    "plt.subplot(1,2,1)\n",
    "abrupt_gmst.plot()\n",
    "plt.xlim(gregory_limits)\n",
    "plt.ylabel('GMST ($^{\\circ}C$)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "abrupt_imbalance.plot()\n",
    "plt.xlim(gregory_limits)\n",
    "plt.ylabel('radiative imbalance (W/m$^{2}$)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = abrupt_imbalance.isel(year=slice(gregory_limits[0],gregory_limits[1]))\n",
    "x_data = abrupt_gmst.isel(year=slice(gregory_limits[0],gregory_limits[1]))\n",
    "a, b = np.polyfit(x_data,y_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(abrupt_gmst, abrupt_imbalance, c = abrupt_gmst.values, cmap='viridis')\n",
    "plt.scatter(x_data, y_data, marker='.', c = 'k', cmap='viridis')\n",
    "x_extrapolate = np.arange(abrupt_gmst.min()-20,abrupt_gmst.min()+20., 0.1)\n",
    "plt.plot(x_extrapolate, a*x_extrapolate + b, color='grey')\n",
    "\n",
    "twoECS = b/(-a)\n",
    "print(\"ECS = \", twoECS/2.)\n",
    "\n",
    "plt.plot([0,twoECS + 1.], [0,0], \"k--\")\n",
    "plt.xlim([0,twoECS + 1.])\n",
    "plt.ylim([-2,8])\n",
    "plt.xlabel('GMST ($^{\\circ}C$)')\n",
    "plt.ylabel('radiative imbalance (W/m$^{2}$)');\n",
    "plt.savefig('../figures/Gregory_method_CESM2_example.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eagerly compute ECS for each model, in preparation for plotting\n",
    "\n",
    "The operations we have done up to this point to calculate the global-mean surface temperature were evaluated lazily. In other worse, we have created a blueprint for how we want to evaluate the calculations, but have not yet computing them. This lets us do things like multiply two 1 Tb arrays together even though they are each individually larger-than-memory.\n",
    "\n",
    "Now we call xarray's `compute()` method to carry out the computations we defined in the for loop above for calculation the global-mean surface temperature anomaly roughly 100-150 years (1200-1800 months) after instantaneous quadrupling of CO2 relative to the last 50 years (600 months) of the control simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECS_dict = {}\n",
    "results = {'ECS': {}, 'ctrl_gmst': {}, 'ctrl_imbalance': {}, 'abrupt_gmst': {}, 'abrupt_imbalance': {}}\n",
    "\n",
    "for name in tqdm(ctrl_gmst_dict.keys()):\n",
    "    results['ctrl_gmst'][name] = ctrl_gmst_dict[name].groupby('time.year').mean(dim='time').compute()\n",
    "    results['ctrl_imbalance'][name] = ctrl_imbalance_dict[name].groupby('time.year').mean(dim='time').compute()\n",
    "    \n",
    "    results['abrupt_gmst'][name] = (\n",
    "        abrupt_gmst_dict[name].groupby('time.year').mean(dim='time') -\n",
    "        results['ctrl_gmst'][name].isel(year=slice(gregory_limits[0],gregory_limits[1])).mean(dim='year')\n",
    "    ).compute()\n",
    "    results['abrupt_imbalance'][name] = (\n",
    "        abrupt_imbalance_dict[name].groupby('time.year').mean(dim='time') -\n",
    "        results['ctrl_imbalance'][name].isel(year=slice(gregory_limits[0],gregory_limits[1])).mean(dim='year')\n",
    "    ).compute()\n",
    "    \n",
    "    # Apply Gregory method to estimate ECS\n",
    "    if results['abrupt_imbalance'][name].size >= gregory_limits[1]:\n",
    "        y_data = results['abrupt_imbalance'][name].isel(year=slice(gregory_limits[0],gregory_limits[1])).compute()\n",
    "        x_data = results['abrupt_gmst'][name].isel(year=slice(gregory_limits[0],gregory_limits[1])).compute()\n",
    "        a, b = np.polyfit(x_data,y_data,1)\n",
    "\n",
    "        results['ECS'][name] = b/(-a) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['ECS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(list(results['ECS'].values())), bins=np.arange(0,max(list(results['ECS'].values()))+1.0,1.0))\n",
    "plt.xlabel(r\"ECS ($^{\\circ}$C)\")\n",
    "plt.ylabel(\"number of models\")\n",
    "plt.title('Equilibrium Climate Sensitivity (ECS) in CMIP6 models')\n",
    "plt.annotate(s=fr\"$N = {len(results['ECS'])}$\",xy=(0.025,0.90), xycoords=\"axes fraction\", fontsize=14)\n",
    "plt.savefig(f\"../figures/ECS_Gregory_{gregory_limits[0]}-{gregory_limits[1]}_hist.png\",dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sorted(results['ECS'].items(), key=lambda x: x[1])\n",
    "ordered_ECS = { pair[0]:pair[1] for pair in tmp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.arange(len(ordered_ECS)), np.array(list(ordered_ECS.values())), align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(ordered_ECS)),ordered_ECS.keys(), rotation=90)\n",
    "plt.ylabel(r\"ECS ($^{\\circ}$C)\")\n",
    "plt.title('Equilibrium Climate Sensitivity (ECS) in CMIP6 models')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../figures/ECS_Gregory_{gregory_limits[0]}-{gregory_limits[1]}_bar.png\",dpi=100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed up your code with dask-kubernetes (if available)\n",
    "```python\n",
    "# Cluster was created via the dask labextension\n",
    "# Delete this cell and replace with a new one\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=10, interval='2s')\n",
    "client = Client(cluster)\n",
    "client\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
